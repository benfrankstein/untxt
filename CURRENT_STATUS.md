# Current System Status & Flow

**Last Updated**: 2025-10-16

---

## âœ… Completed Phases

### **Phase 1: Data Layer** âœ… COMPLETE
- **PostgreSQL Database** (localhost:5432)
  - Users, files, tasks, results tables
  - 2 migrations applied (S3 fields added)
  - Admin user seeded
- **Redis** (localhost:6379)
  - Task queue: `ocr:task:queue`
  - Pub/Sub: `ocr:notifications` channel
  - Task metadata storage
- **AWS S3 Storage** (HIPAA-compliant)
  - Bucket: `untxt`
  - KMS encryption enabled
  - Hierarchical folder structure

### **Phase 2: Processing Layer** âœ… COMPLETE
- **Flask Worker** (localhost:5000)
  - Health check endpoint: `/health`
  - Status endpoint: `/status`
- **Simulated Qwen3 Model**
  - Returns German bakery receipt HTML
  - 1-3 second processing time
  - Realistic confidence scores
- **S3 Integration** âœ… NEW!
  - Downloads input files from S3
  - Uploads HTML results to S3
  - Stores S3 paths in database
  - No local file persistence
- **Task Processing Pipeline**
  - Redis queue consumer
  - Full OCR workflow
  - Error handling & retry logic
  - Real-time notifications

---

## ğŸš§ Remaining Phases

### **Phase 3: Backend API** â³ NOT STARTED
- Node.js Express server (will run on localhost:8080)
- REST API endpoints for task management
- WebSocket server for real-time updates
- User authentication

### **Phase 4: Frontend** â³ NOT STARTED
- Next.js web interface (will run on localhost:3000)
- File upload UI
- Task dashboard
- Real-time status updates

### **Phase 5: Admin UI** â³ NOT STARTED
- Electron desktop app
- System monitoring
- User management

### **Phase 6: Reverse Proxy** â³ NOT STARTED
- Nginx configuration
- SSL/TLS termination
- Rate limiting

### **Phase 7-10**: Integration Testing, Production Deployment, Monitoring, Scaling

---

## ğŸ”„ Current System Flow

### **Architecture Overview**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        AWS S3 (untxt)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   uploads/               â”‚  â”‚   results/              â”‚     â”‚
â”‚  â”‚   {user}/{date}/{file}   â”‚  â”‚   {user}/{date}/{task}  â”‚     â”‚
â”‚  â”‚   [KMS Encrypted]        â”‚  â”‚   [KMS Encrypted]       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ â†‘ download/upload             â”‚ â†‘ upload result
              â”‚ â”‚                             â”‚ â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2     â”‚ â”‚                             â”‚ â”‚   PROCESSING    â”‚
â”‚             â†“ â”‚                             â”‚ â†“                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚       OCR Worker (Flask + Python)                â”‚          â”‚
â”‚  â”‚       Port: 5000                                 â”‚          â”‚
â”‚  â”‚       PID: 66564                                 â”‚          â”‚
â”‚  â”‚                                                   â”‚          â”‚
â”‚  â”‚  1. Polls Redis queue                            â”‚          â”‚
â”‚  â”‚  2. Downloads file from S3                       â”‚          â”‚
â”‚  â”‚  3. Runs simulated Qwen3 (German receipt HTML)   â”‚          â”‚
â”‚  â”‚  4. Uploads result HTML to S3                    â”‚          â”‚
â”‚  â”‚  5. Stores S3 paths in PostgreSQL                â”‚          â”‚
â”‚  â”‚  6. Publishes completion to Redis Pub/Sub        â”‚          â”‚
â”‚  â”‚  7. Cleans up temp files                         â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚              â”‚ read/write            â”‚ BRPOP/PUBLISH           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1      â†“                       â†“          DATA LAYER     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   PostgreSQL           â”‚  â”‚   Redis                 â”‚      â”‚
â”‚  â”‚   Port: 5432           â”‚  â”‚   Port: 6379            â”‚      â”‚
â”‚  â”‚                        â”‚  â”‚                         â”‚      â”‚
â”‚  â”‚  Tables:               â”‚  â”‚  Queues:                â”‚      â”‚
â”‚  â”‚  â€¢ users               â”‚  â”‚  â€¢ ocr:task:queue       â”‚      â”‚
â”‚  â”‚  â€¢ files (+ s3_key)    â”‚  â”‚                         â”‚      â”‚
â”‚  â”‚  â€¢ tasks               â”‚  â”‚  Pub/Sub:               â”‚      â”‚
â”‚  â”‚  â€¢ results             â”‚  â”‚  â€¢ ocr:notifications    â”‚      â”‚
â”‚  â”‚    (+ s3_result_key)   â”‚  â”‚                         â”‚      â”‚
â”‚  â”‚  â€¢ sessions, history   â”‚  â”‚  Metadata:              â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â€¢ ocr:task:data:{id}   â”‚      â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Detailed Processing Flow

### **1. Task Creation** (Currently manual, will be via API in Phase 3)

```sql
-- Manual test creates:
INSERT INTO files (id, user_id, original_filename, s3_key, ...)
VALUES ('file-uuid', 'user-uuid', 'document.pdf', 'uploads/user/2025-10/file/document.pdf', ...);

INSERT INTO tasks (id, user_id, file_id, status)
VALUES ('task-uuid', 'user-uuid', 'file-uuid', 'pending');

-- Add to Redis queue
LPUSH ocr:task:queue "task-uuid"
```

### **2. Worker Processing** (Automated, running now)

```
Worker Loop (continuous):
â”œâ”€ 1. BRPOP ocr:task:queue (blocking wait, 5 sec timeout)
â”‚  â””â”€ Gets: task-uuid
â”‚
â”œâ”€ 2. Update status to 'processing'
â”‚  â”œâ”€ PostgreSQL: UPDATE tasks SET status='processing'
â”‚  â””â”€ Redis: SET ocr:task:data:{task-uuid} {status, worker_id, started_at}
â”‚
â”œâ”€ 3. Fetch task details
â”‚  â””â”€ PostgreSQL: SELECT tasks JOIN files (gets s3_key)
â”‚
â”œâ”€ 4. Download file from S3
â”‚  â”œâ”€ S3: GET s3://untxt/uploads/user/2025-10/file/document.pdf
â”‚  â””â”€ Save to: /output/temp/{task-uuid}_document.pdf
â”‚
â”œâ”€ 5. Run OCR (simulated Qwen3)
â”‚  â”œâ”€ Loads file (simulated)
â”‚  â”œâ”€ Sleep 1-3 seconds (simulates processing)
â”‚  â””â”€ Returns: {html_output, confidence, word_count, page_count}
â”‚
â”œâ”€ 6. Upload result to S3
â”‚  â”œâ”€ S3: PUT s3://untxt/results/user/2025-10/task/{task-uuid}.html
â”‚  â””â”€ KMS encryption applied automatically
â”‚
â”œâ”€ 7. Store result in database
â”‚  â””â”€ PostgreSQL: INSERT INTO results (task_id, extracted_text, s3_result_key, ...)
â”‚
â”œâ”€ 8. Update status to 'completed'
â”‚  â”œâ”€ PostgreSQL: UPDATE tasks SET status='completed', completed_at=now()
â”‚  â””â”€ Redis: UPDATE ocr:task:data:{task-uuid}
â”‚
â”œâ”€ 9. Publish notification
â”‚  â”œâ”€ Redis: PUBLISH ocr:notifications {task completed event}
â”‚  â””â”€ Redis: PUBLISH ocr:notifications:user:{user-uuid} {user-specific event}
â”‚
â”œâ”€ 10. Cleanup
â”‚  â”œâ”€ Delete temp file: /output/temp/{task-uuid}_document.pdf
â”‚  â””â”€ Set Redis key expiry: 86400 seconds (24 hours)
â”‚
â””â”€ 11. Loop back to step 1
```

---

## ğŸ—‚ï¸ File Organization

### **S3 Bucket Structure**

```
s3://untxt/
â”œâ”€â”€ uploads/                          # Input files
â”‚   â””â”€â”€ {user_id}/                    # User partition
â”‚       â””â”€â”€ {YYYY-MM}/                # Date partition
â”‚           â””â”€â”€ {file_id}/            # File partition
â”‚               â””â”€â”€ {filename}        # Original filename
â”‚                   Example: test_receipt.pdf
â”‚
â””â”€â”€ results/                          # Output files
    â””â”€â”€ {user_id}/                    # User partition
        â””â”€â”€ {YYYY-MM}/                # Date partition
            â””â”€â”€ {task_id}/            # Task partition
                â””â”€â”€ {task_id}_{timestamp}.html
                    Example: 6d6daecf-8cbb-4b1c-8f9b-717e71a0b50d_20251016_214250.html
```

### **Database Storage**

```sql
-- Files table stores S3 path for input
files:
  - id: UUID
  - user_id: UUID
  - original_filename: "document.pdf"
  - file_path: NULL (no local storage)
  - s3_key: "uploads/user-id/2025-10/file-id/document.pdf"
  - file_type: "pdf"
  - uploaded_at: timestamp

-- Results table stores S3 path for output
results:
  - id: UUID
  - task_id: UUID
  - extracted_text: "full text extraction"
  - confidence_score: 0.92
  - result_file_path: NULL (no local storage)
  - s3_result_key: "results/user-id/2025-10/task-id/result.html"
  - created_at: timestamp
```

---

## ğŸ” Security & Encryption

### **Data Encryption Flow**

```
User File Upload (future Phase 3)
    â†“
Backend API receives file
    â†“
Upload to S3 with KMS encryption
    â†“
S3 â†’ KMS: "Encrypt this file"
    â†“
KMS generates unique Data Encryption Key (DEK)
    â†“
S3 encrypts file with DEK
    â†“
KMS encrypts DEK with Master Key
    â†“
S3 stores:
  - Encrypted file
  - Encrypted DEK
  - KMS Master Key ARN
    â†“
File at rest: âœ… Encrypted
```

### **Current Credentials** (Development)

```bash
# worker/.env (NEVER commit to git!)
AWS_ACCESS_KEY_ID=AKIAX45MOZ3HGNYUTJUS                    # âš ï¸ ROTATE BEFORE PRODUCTION
AWS_SECRET_ACCESS_KEY=stusVT4eFa...                       # âš ï¸ ROTATE BEFORE PRODUCTION
AWS_REGION=us-east-1
S3_BUCKET_NAME=untxt
KMS_KEY_ID=arn:aws:kms:us-east-1:543138172622:key/e608ce2c-...
```

**Production**: See `docs/PRODUCTION_SECURITY.md` for IAM Roles or Secrets Manager setup.

---

## ğŸ“ Testing

### **Test Scripts Available**

1. **`test_worker.sh`** - Basic worker test (without S3)
   ```bash
   ./test_worker.sh
   ```

2. **`test_s3_integration.sh`** âœ… NEW!
   ```bash
   ./test_s3_integration.sh
   ```
   - Creates test file
   - Uploads to S3
   - Creates database records
   - Enqueues task
   - Waits for completion
   - Verifies result in S3
   - Validates encryption

### **Service Management**

```bash
# Start all services (PostgreSQL, Redis, Worker, Flask)
./start_services.sh

# Check status
./status.sh

# Stop all services
./stop_services.sh

# View logs
tail -f logs/worker.log
tail -f logs/flask.log
```

---

## ğŸ¯ Next Steps (Phase 3)

To continue development, you'll need to build the **Backend API**:

### **What to Build Next**

1. **Node.js Express Server**
   - REST API for file uploads
   - Task management endpoints
   - User authentication
   - WebSocket server for real-time updates

2. **Key Endpoints Needed**
   ```
   POST   /api/auth/register          # User registration
   POST   /api/auth/login             # User login
   POST   /api/tasks                  # Upload file, create task
   GET    /api/tasks                  # List user's tasks
   GET    /api/tasks/:id              # Get task details
   GET    /api/tasks/:id/result       # Download result HTML
   WS     /ws                         # WebSocket for real-time updates
   ```

3. **Backend Will Need To**
   - Accept file uploads from frontend
   - Upload files to S3 (using your S3 client pattern)
   - Create database records (files, tasks tables)
   - Push task IDs to Redis queue: `LPUSH ocr:task:queue {task-uuid}`
   - Subscribe to Redis Pub/Sub for completion notifications
   - Broadcast status updates via WebSocket to frontend

### **Dependencies Between Phases**

```
Phase 1 (Data) â”€â”¬â”€â†’ Phase 2 (Worker) âœ… COMPLETE
                â”‚
                â””â”€â†’ Phase 3 (Backend API) â³ NEXT
                    â”‚
                    â””â”€â†’ Phase 4 (Frontend) â³ AFTER PHASE 3
```

---

## ğŸ“ˆ System Metrics (Current Test Run)

From latest S3 integration test:

```
Task ID: 6d6daecf-8cbb-4b1c-8f9b-717e71a0b50d
â”œâ”€ File uploaded to S3: âœ… (283 ms)
â”œâ”€ Database records created: âœ…
â”œâ”€ Task queued: âœ…
â”œâ”€ Worker picked up task: âœ… (0.5 sec)
â”œâ”€ File downloaded from S3: âœ… (283 ms)
â”œâ”€ OCR processing: âœ… (2.73 sec)
â”œâ”€ Result uploaded to S3: âœ… (116 ms)
â”œâ”€ Database updated: âœ…
â”œâ”€ Notification published: âœ…
â””â”€ Total time: ~5 seconds

Result Details:
â”œâ”€ Confidence Score: 0.9190
â”œâ”€ Word Count: 37
â”œâ”€ Page Count: 2
â”œâ”€ Processing Time: 2730ms
â”œâ”€ Result Size: 8464 bytes
â”œâ”€ Encryption: aws:kms âœ…
â””â”€ Storage: S3 only (no local files)
```

---

## ğŸš€ Quick Start (Current System)

```bash
# 1. Start services
./start_services.sh

# 2. Run S3 integration test
./test_s3_integration.sh

# 3. Check logs
tail -f logs/worker.log

# 4. View results
# - Check S3 bucket in AWS console
# - Query PostgreSQL: SELECT * FROM results ORDER BY created_at DESC LIMIT 1;
# - Download HTML from S3 using s3_result_key
```

---

## ğŸ“š Documentation

- **Development Plan**: `development_order.md`
- **Phase 2 Complete**: `PHASE2_COMPLETE.md`
- **Service Scripts**: `SERVICE_SCRIPTS.md`
- **AWS S3 Setup**: `docs/AWS_S3_HIPAA_SETUP.md`
- **Production Security**: `docs/PRODUCTION_SECURITY.md` âœ… NEW!
- **Expected Output**: `EXPECTED_OUTPUT.md`

---

## ğŸ‰ Summary

**You have a fully functional OCR processing backend!**

âœ… Data layer with PostgreSQL + Redis
âœ… OCR worker with simulated Qwen3 model
âœ… AWS S3 integration with KMS encryption
âœ… HIPAA-compliant file storage
âœ… Complete task processing pipeline
âœ… End-to-end testing

**Next**: Build the Backend API (Phase 3) to accept file uploads from a frontend.
